{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# get the data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\n",
    "    \"/tmp/tensorflow/mnist/input_data\", \n",
    "    reshape=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap value function\n",
    "# for label and image infomation\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "# for image it self\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the full path to store data\n",
    "def _data_path(data_directory,name):\n",
    "    if not os.path.isdir(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "    return os.path.join(data_directory,f'{name}.tfrecords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to(images,labels,name,data_direcotory,num_shards=1):\n",
    "    \n",
    "    num_examples, rows, cols, depth = images.shape\n",
    "    data_set = list(zip(images,labels))\n",
    "    \n",
    "    def _process_examples(example_dataset,filename):\n",
    "        print(f'Processing {filename} data')\n",
    "        dataset_length = len(example_dataset)\n",
    "        with tf.python_io.TFRecordWriter(filename) as writer:\n",
    "            for index, (image, label) in enumerate(example_dataset):\n",
    "                sys.stdout.write(f\"\\rProcessing sample {index+1} of {dataset_length}\")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                image_raw = image.tostring()\n",
    "                example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                    'height': _int64_feature(rows),\n",
    "                    'width': _int64_feature(cols),\n",
    "                    'depth': _int64_feature(depth),\n",
    "                    'label': _int64_feature(int(label)),\n",
    "                    'image_raw': _bytes_feature(image_raw)\n",
    "                }))\n",
    "                writer.write(example.SerializeToString())\n",
    "            print()\n",
    "    \n",
    "    if num_shards == 1:\n",
    "        _process_examples(data_set, _data_path(data_directory, name))\n",
    "    else:\n",
    "        sharded_dataset = np.array_split(data_set, num_shards)\n",
    "        for shard, dataset in enumerate(sharded_dataset):\n",
    "            _process_examples(dataset, _data_path(data_directory, f'{name}-{shard+1}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.concatenate([mnist.train.images,mnist.validation.images,mnist.test.images],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.concatenate([mnist.train.labels,mnist.validation.labels,mnist.test.labels],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing mnist/data-1.tfrecords data\n",
      "Processing sample 5834 of 5834\n",
      "Processing mnist/data-2.tfrecords data\n",
      "Processing sample 5834 of 5834\n",
      "Processing mnist/data-4.tfrecords data\n",
      "Processing sample 5834 of 5834\n",
      "Processing mnist/data-5.tfrecords data\n",
      "Processing sample 5833 of 5833\n",
      "Processing mnist/data-7.tfrecords data\n",
      "Processing sample 5833 of 5833\n",
      "Processing mnist/data-8.tfrecords data\n",
      "Processing sample 5833 of 5833\n",
      "Processing mnist/data-9.tfrecords data\n",
      "Processing sample 5833 of 5833\n",
      "Processing mnist/data-10.tfrecords data\n",
      "Processing sample 5833 of 5833\n",
      "Processing mnist/data-11.tfrecords data\n",
      "Processing sample 5833 of 5833\n",
      "Processing mnist/data-12.tfrecords data\n",
      "Processing sample 5833 of 5833\n"
     ]
    }
   ],
   "source": [
    "data_directory = \"mnist\"\n",
    "convert_to(images,labels, 'data', data_directory, num_shards=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
