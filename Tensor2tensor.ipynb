{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本导入和设定\n",
    "# Imports we need.\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "\n",
    "from tensor2tensor import models\n",
    "from tensor2tensor import problems\n",
    "from tensor2tensor.layers import common_layers\n",
    "from tensor2tensor.utils import trainer_lib\n",
    "from tensor2tensor.utils import t2t_model\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.utils import metrics\n",
    "\n",
    "# Enable TF Eager execution\n",
    "tfe = tf.contrib.eager\n",
    "tfe.enable_eager_execution()\n",
    "\n",
    "# Other setup\n",
    "Modes = tf.estimator.ModeKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新建文件夹设定\n",
    "DATA_DIR=\"t2t/data\"\n",
    "TMP_DIR=\"t2t/tmp\"\n",
    "TRAIN_DIR=\"t2t/train\"\n",
    "CHECKPOINT_DIR=\"t2t/checkpoints\"\n",
    "# Setup some directories\n",
    "data_dir = DATA_DIR\n",
    "tmp_dir = TMP_DIR\n",
    "train_dir = TRAIN_DIR\n",
    "checkpoint_dir = CHECKPOINT_DIR\n",
    "\n",
    "tf.gfile.MakeDirs(data_dir)\n",
    "tf.gfile.MakeDirs(tmp_dir)\n",
    "tf.gfile.MakeDirs(train_dir)\n",
    "tf.gfile.MakeDirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取问题和生成数据\n",
    "# Fetch the librispeech_clean problem\n",
    "librispeech_clean = problems.problem(\"librispeech_clean\")\n",
    "# The generate_data method of a problem will download data and process it into\n",
    "# a standard format ready for training and evaluation.\n",
    "librispeech_clean.generate_data(data_dir, tmp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "class MySimpleModel(t2t_model.T2TModel):\n",
    "\n",
    "  def body(self, features):\n",
    "    inputs = features[\"inputs\"]\n",
    "    filters = self.hparams.hidden_size\n",
    "    h1 = tf.layers.conv2d(inputs, filters,\n",
    "                          kernel_size=(5, 5), strides=(2, 2))\n",
    "    h2 = tf.layers.conv2d(tf.nn.relu(h1), filters,\n",
    "                          kernel_size=(5, 5), strides=(2, 2))\n",
    "    return tf.layers.conv2d(tf.nn.relu(h2), filters,\n",
    "                            kernel_size=(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定超参数和模型\n",
    "hparams = trainer_lib.create_hparams(\"basic_1\", data_dir=data_dir, problem_name=\"image_mnist\")\n",
    "hparams.hidden_size = 64\n",
    "model = MySimpleModel(hparams, Modes.TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eager model 设定loss batchsiz和优化器\n",
    "# Prepare for the training loop\n",
    "\n",
    "# In Eager mode, opt.minimize must be passed a loss function wrapped with\n",
    "# implicit_value_and_gradients\n",
    "@tfe.implicit_value_and_gradients\n",
    "def loss_fn(features):\n",
    "  _, losses = model(features)\n",
    "  return losses[\"training\"]\n",
    "\n",
    "# Setup the training data\n",
    "BATCH_SIZE = 128\n",
    "mnist_train_dataset = mnist_problem.dataset(Modes.TRAIN, data_dir)\n",
    "mnist_train_dataset = mnist_train_dataset.repeat(None).batch(BATCH_SIZE)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "# Train\n",
    "NUM_STEPS = 500\n",
    "\n",
    "for count, example in enumerate(tfe.Iterator(mnist_train_dataset)):\n",
    "  example[\"targets\"] = tf.reshape(example[\"targets\"], [BATCH_SIZE, 1, 1, 1])  # Make it 4D.\n",
    "  loss, gv = loss_fn(example)\n",
    "  optimizer.apply_gradients(gv)\n",
    "\n",
    "  if count % 50 == 0:\n",
    "    print(\"Step: %d, Loss: %.3f\" % (count, loss.numpy()))\n",
    "  if count >= NUM_STEPS:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型\n",
    "model.set_mode(Modes.EVAL)\n",
    "mnist_eval_dataset = mnist_problem.dataset(Modes.EVAL, data_dir)\n",
    "\n",
    "# Create eval metric accumulators for accuracy (ACC) and accuracy in\n",
    "# top 5 (ACC_TOP5)\n",
    "metrics_accum, metrics_result = metrics.create_eager_metrics(\n",
    "    [metrics.Metrics.ACC, metrics.Metrics.ACC_TOP5])\n",
    "\n",
    "for count, example in enumerate(tfe.Iterator(mnist_eval_dataset)):\n",
    "  if count >= 200:\n",
    "    break\n",
    "\n",
    "  # Make the inputs and targets 4D\n",
    "  example[\"inputs\"] = tf.reshape(example[\"inputs\"], [1, 28, 28, 1])\n",
    "  example[\"targets\"] = tf.reshape(example[\"targets\"], [1, 1, 1, 1])\n",
    "\n",
    "  # Call the model\n",
    "  predictions, _ = model(example)\n",
    "\n",
    "  # Compute and accumulate metrics\n",
    "  metrics_accum(predictions, example[\"targets\"])\n",
    "\n",
    "# Print out the averaged metric values on the eval data\n",
    "for name, val in metrics_result().items():\n",
    "  print(\"%s: %.2f\" % (name, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义模型\n",
    "%%writefile poetry/trainer/problem.py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.models import transformer\n",
    "from tensor2tensor.data_generators import problem\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "from tensor2tensor.data_generators import text_problems\n",
    "from tensor2tensor.data_generators import generator_utils\n",
    "\n",
    "#定义问题\n",
    "@registry.register_problem\n",
    "class PoetryLineProblem(text_problems.Text2TextProblem):\n",
    "  \"\"\"Predict next line of poetry from the last line. From Gutenberg texts.\"\"\"\n",
    "\n",
    "  @property\n",
    "  def approx_vocab_size(self):\n",
    "    return 2**13  # ~8k\n",
    "\n",
    "  @property\n",
    "  def is_generate_per_split(self):\n",
    "    # generate_data will NOT shard the data into TRAIN and EVAL for us.\n",
    "    return False\n",
    "\n",
    "  @property\n",
    "  def dataset_splits(self):\n",
    "    \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n",
    "    # 10% evaluation data\n",
    "    return [{\n",
    "        \"split\": problem.DatasetSplit.TRAIN,\n",
    "        \"shards\": 90,\n",
    "    }, {\n",
    "        \"split\": problem.DatasetSplit.EVAL,\n",
    "        \"shards\": 10,\n",
    "    }]\n",
    "\n",
    "  def generate_samples(self, data_dir, tmp_dir, dataset_split):\n",
    "    with open('data/poetry/raw.txt', 'r') as rawfp:\n",
    "      prev_line = ''\n",
    "      for curr_line in rawfp:\n",
    "        curr_line = curr_line.strip()\n",
    "        # poems break at empty lines, so this ensures we train only\n",
    "        # on lines of the same poem\n",
    "        if len(prev_line) > 0 and len(curr_line) > 0:       \n",
    "            yield {\n",
    "                \"inputs\": prev_line,\n",
    "                \"targets\": curr_line\n",
    "            }\n",
    "        prev_line = curr_line          \n",
    "\n",
    "\n",
    "# 定义超参数\n",
    "# Smaller than the typical translate model, and with more regularization\n",
    "@registry.register_hparams\n",
    "def transformer_poetry():\n",
    "  hparams = transformer.transformer_base()\n",
    "  hparams.num_hidden_layers = 2\n",
    "  hparams.hidden_size = 128\n",
    "  hparams.filter_size = 512\n",
    "  hparams.num_heads = 4\n",
    "  hparams.attention_dropout = 0.6\n",
    "  hparams.layer_prepostprocess_dropout = 0.6\n",
    "  hparams.learning_rate = 0.05\n",
    "  return hparams\n",
    "\n",
    "# 定义超参数范围\n",
    "# hyperparameter tuning ranges\n",
    "@registry.register_ranged_hparams\n",
    "def transformer_poetry_range(rhp):\n",
    "  rhp.set_float(\"learning_rate\", 0.05, 0.25, scale=rhp.LOG_SCALE)\n",
    "  rhp.set_int(\"num_hidden_layers\", 2, 4)\n",
    "  rhp.set_discrete(\"hidden_size\", [128, 256, 512])\n",
    "  rhp.set_float(\"attention_dropout\", 0.4, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入problem文件结构\n",
    "%%writefile poetry/trainer/__init__.py\n",
    "from . import problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入其他结构文件\n",
    "%%writefile poetry/setup.py\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "  'tensor2tensor'\n",
    "]\n",
    "\n",
    "setup(\n",
    "    name='poetry',\n",
    "    version='0.1',\n",
    "    author = 'Google',\n",
    "    author_email = 'training-feedback@cloud.google.com',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Poetry Line Problem',\n",
    "    requires=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 继续结构文件\n",
    "!touch  poetry/__init__ .py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成数据\n",
    "%%bash\n",
    "DATA_DIR=\"./t2t_data\"\n",
    "TMP_DIR=\"$DATA_DIR/tmp\"\n",
    "rm -rf $DATA_DIR $TMP_DIR\n",
    "mkdir -p ./t2t_data ./t2t_data/tmp\n",
    "# Generate data\n",
    "t2t-datagen \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --data_dir=$DATA_DIR \\\n",
    "  --tmp_dir=$TMP_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练数据\n",
    "\n",
    "%%bash\n",
    "DATA_DIR=\"./t2t_data\"\n",
    "OUTDIR=\"./trained_model\"\n",
    "rm -rf $OUTDIR\n",
    "mkdir $OUTDIR\n",
    "t2t-trainer \\\n",
    "  --data_dir=$DATA_DIR \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --model=transformer \\\n",
    "  --hparams_set=transformer_poetry \\\n",
    "  --output_dir=$OUTDIR --job-dir=$OUTDIR --train_steps=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode数据\n",
    "%%bash\n",
    "# same as the above training job ...\n",
    "OUTDIR=\"./trained_model\"#_full2  # or ${TOPDIR}/poetry/model_full\n",
    "DATA_DIR=\"./t2t_data\"\n",
    "MODEL=\"transformer\"\n",
    "HPARAMS=\"transformer_poetry\"\n",
    "# this is what this notebook is demonstrating\n",
    "PROBLEM= 'poetry_line_problem'\n",
    "\n",
    "# the file with the input lines\n",
    "DECODE_FILE=data/poetry/rumi_leads.txt\n",
    "\n",
    "BEAM_SIZE=4\n",
    "ALPHA=0.6\n",
    "\n",
    "t2t-decoder \\\n",
    "  --data_dir=$DATA_DIR \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --model=$MODEL \\\n",
    "  --hparams_set=$HPARAMS \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --decode_hparams=\"beam_size=$BEAM_SIZE,alpha=$ALPHA\" \\\n",
    "  --decode_from_file=$DECODE_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看decode数据\n",
    "%%bash  \n",
    "DECODE_FILE=data/poetry/rumi_leads.txt\n",
    "cat ${DECODE_FILE}.*.decodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导出模型\n",
    "%%bash\n",
    "OUTDIR=\"./trained_model\"\n",
    "DATA_DIR=\"./t2t_data\"\n",
    "MODEL=transformer\n",
    "HPARAMS=transformer_poetry\n",
    "BEAM_SIZE=4\n",
    "ALPHA=0.6\n",
    "\n",
    "t2t-exporter \\\n",
    "  --model=$MODEL \\\n",
    "  --hparams_set=$HPARAMS \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --decode_hparams=\"beam_size=$BEAM_SIZE,alpha=$ALPHA\" \\\n",
    "  --data_dir=$DATA_DIR \\\n",
    "  --output_dir=$OUTDIR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
