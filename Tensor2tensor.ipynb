{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本导入和设定\n",
    "# Imports we need.\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "\n",
    "from tensor2tensor import models\n",
    "from tensor2tensor import problems\n",
    "from tensor2tensor.layers import common_layers\n",
    "from tensor2tensor.utils import trainer_lib\n",
    "from tensor2tensor.utils import t2t_model\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.utils import metrics\n",
    "\n",
    "# Enable TF Eager execution\n",
    "tfe = tf.contrib.eager\n",
    "tfe.enable_eager_execution()\n",
    "\n",
    "# Other setup\n",
    "Modes = tf.estimator.ModeKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新建文件夹设定\n",
    "DATA_DIR=\"t2t/data\"\n",
    "TMP_DIR=\"t2t/tmp\"\n",
    "TRAIN_DIR=\"t2t/train\"\n",
    "CHECKPOINT_DIR=\"t2t/checkpoints\"\n",
    "# Setup some directories\n",
    "data_dir = DATA_DIR\n",
    "tmp_dir = TMP_DIR\n",
    "train_dir = TRAIN_DIR\n",
    "checkpoint_dir = CHECKPOINT_DIR\n",
    "\n",
    "tf.gfile.MakeDirs(data_dir)\n",
    "tf.gfile.MakeDirs(tmp_dir)\n",
    "tf.gfile.MakeDirs(train_dir)\n",
    "tf.gfile.MakeDirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取问题和生成数据\n",
    "# Fetch the librispeech_clean problem\n",
    "librispeech_clean = problems.problem(\"librispeech_clean\")\n",
    "# The generate_data method of a problem will download data and process it into\n",
    "# a standard format ready for training and evaluation.\n",
    "librispeech_clean.generate_data(data_dir, tmp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "class MySimpleModel(t2t_model.T2TModel):\n",
    "\n",
    "  def body(self, features):\n",
    "    inputs = features[\"inputs\"]\n",
    "    filters = self.hparams.hidden_size\n",
    "    h1 = tf.layers.conv2d(inputs, filters,\n",
    "                          kernel_size=(5, 5), strides=(2, 2))\n",
    "    h2 = tf.layers.conv2d(tf.nn.relu(h1), filters,\n",
    "                          kernel_size=(5, 5), strides=(2, 2))\n",
    "    return tf.layers.conv2d(tf.nn.relu(h2), filters,\n",
    "                            kernel_size=(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定超参数和模型\n",
    "hparams = trainer_lib.create_hparams(\"basic_1\", data_dir=data_dir, problem_name=\"image_mnist\")\n",
    "hparams.hidden_size = 64\n",
    "model = MySimpleModel(hparams, Modes.TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eager model 设定loss batchsiz和优化器\n",
    "# Prepare for the training loop\n",
    "\n",
    "# In Eager mode, opt.minimize must be passed a loss function wrapped with\n",
    "# implicit_value_and_gradients\n",
    "@tfe.implicit_value_and_gradients\n",
    "def loss_fn(features):\n",
    "  _, losses = model(features)\n",
    "  return losses[\"training\"]\n",
    "\n",
    "# Setup the training data\n",
    "BATCH_SIZE = 128\n",
    "mnist_train_dataset = mnist_problem.dataset(Modes.TRAIN, data_dir)\n",
    "mnist_train_dataset = mnist_train_dataset.repeat(None).batch(BATCH_SIZE)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "# Train\n",
    "NUM_STEPS = 500\n",
    "\n",
    "for count, example in enumerate(tfe.Iterator(mnist_train_dataset)):\n",
    "  example[\"targets\"] = tf.reshape(example[\"targets\"], [BATCH_SIZE, 1, 1, 1])  # Make it 4D.\n",
    "  loss, gv = loss_fn(example)\n",
    "  optimizer.apply_gradients(gv)\n",
    "\n",
    "  if count % 50 == 0:\n",
    "    print(\"Step: %d, Loss: %.3f\" % (count, loss.numpy()))\n",
    "  if count >= NUM_STEPS:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型\n",
    "model.set_mode(Modes.EVAL)\n",
    "mnist_eval_dataset = mnist_problem.dataset(Modes.EVAL, data_dir)\n",
    "\n",
    "# Create eval metric accumulators for accuracy (ACC) and accuracy in\n",
    "# top 5 (ACC_TOP5)\n",
    "metrics_accum, metrics_result = metrics.create_eager_metrics(\n",
    "    [metrics.Metrics.ACC, metrics.Metrics.ACC_TOP5])\n",
    "\n",
    "for count, example in enumerate(tfe.Iterator(mnist_eval_dataset)):\n",
    "  if count >= 200:\n",
    "    break\n",
    "\n",
    "  # Make the inputs and targets 4D\n",
    "  example[\"inputs\"] = tf.reshape(example[\"inputs\"], [1, 28, 28, 1])\n",
    "  example[\"targets\"] = tf.reshape(example[\"targets\"], [1, 1, 1, 1])\n",
    "\n",
    "  # Call the model\n",
    "  predictions, _ = model(example)\n",
    "\n",
    "  # Compute and accumulate metrics\n",
    "  metrics_accum(predictions, example[\"targets\"])\n",
    "\n",
    "# Print out the averaged metric values on the eval data\n",
    "for name, val in metrics_result().items():\n",
    "  print(\"%s: %.2f\" % (name, val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
