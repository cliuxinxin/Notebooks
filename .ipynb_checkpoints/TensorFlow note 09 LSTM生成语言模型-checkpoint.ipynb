{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前排定义一下训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 训练循环次数\n",
    "num_epochs = 10\n",
    "\n",
    "# batch大小\n",
    "batch_size = 256\n",
    "\n",
    "# lstm层中包含的unit个数\n",
    "rnn_size = 256\n",
    "\n",
    "# lstm层数\n",
    "num_layers = 3\n",
    "\n",
    "# embedding layer的大小\n",
    "embed_dim = 300\n",
    "\n",
    "# 训练步长\n",
    "seq_length = 30\n",
    "\n",
    "# 学习率\n",
    "learning_rate = 0.001\n",
    "\n",
    "#dropout keep\n",
    "output_keep_prob = 0.8\n",
    "input_keep_prob = 1.0\n",
    "\n",
    "# 优化器\n",
    "grad_clip = 5.\n",
    "\n",
    "decay_rate = 0.97\n",
    "init_from = None\n",
    "save_every = 1000\n",
    "# 保存模型\n",
    "save_dir = './save'\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "# 保存logs   \n",
    "log_dir = './logs'\n",
    "if not os.path.isdir(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# 保存数据和词汇\n",
    "data_dir = './temp'\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    \n",
    "input_file = os.path.join(data_dir, \"爵迹I II.txt\")\n",
    "vocab_file = os.path.join(data_dir, \"vocab.pkl\")\n",
    "tensor_file = os.path.join(data_dir, \"data.npy\")\n",
    "vocab_file = os.path.join(save_dir, 'chars_vocab.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先加载数据集 <p>\n",
    "使用到的是**`爵迹`**这本小说<p>\n",
    "无论小说和电影都能给人很深刻的印象...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xa1 in position 4: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7ae348020866>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xa1 in position 4: invalid start byte"
     ]
    }
   ],
   "source": [
    "with open(input_file, 'r') as f:\n",
    "        text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预览一下部分内容<p>**\n",
    "果然一股东方神话、字里行间透露出45度角仰望天空的忧伤气息扑面而来<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[500:800]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 做一些数据预处理，去掉一写无关的字符和空格，去掉书籍前几行没用的介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = re.compile('\\[.*\\]|<.*>|\\.+|【|】| +|\\\\r|\\\\n')\n",
    "text = pattern.sub('', text.strip()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[500:800]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  感觉预处理后效果还可以.没那么乱了，开始做词映射\n",
    "1. 首先做词频统计，再降序排序，因为用的是char级的所以这一步是没什么必要的，统计有多少个汉字和字符，其实可以用``chars=set(text)``代替\n",
    "2. 将统计结果作为语料库，存入本地pkl文件中，方便调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from six.moves import cPickle\n",
    "counter = collections.Counter(text)\n",
    "counter = sorted(counter.items(), key=lambda x: -x[1])\n",
    "chars, _  = zip(*counter)\n",
    "with open(vocab_file, 'wb') as f:\n",
    "    cPickle.dump(chars, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对词汇表字符(包括\\n哦)做一个数字索引，并用这个数字索引来代替这个汉字<p>\n",
    "保存字词映射表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(chars)\n",
    "vocab = dict(zip(chars, range(vocab_size)))\n",
    "with open(vocab_file, 'wb') as f:\n",
    "    cPickle.dump((chars, vocab), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 将整本书的内容，做一下 汉字/字符  - 数字 的变化。\n",
    "2. 这样原来的一本书变可以用一个由N个数字组成的列表表示了\n",
    "3. 最后把向量化的这本书保存下来，方便之后调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "text_tensor = np.array(list(map(vocab.get, text)))\n",
    "np.save(tensor_file, text_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建训练所需数据格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = int(text_tensor.size / (batch_size * seq_length))\n",
    "\n",
    "if num_batches == 0:\n",
    "    assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
    "\n",
    "text_tensor = text_tensor[: num_batches * batch_size * seq_length]\n",
    "xdata = text_tensor\n",
    "ydata = np.copy(text_tensor)\n",
    "\n",
    "#循环神经网络，最后一个输出为最先一个输入\n",
    "ydata[:-1] = xdata[1:]\n",
    "ydata[-1] = xdata[0]\n",
    "x_batches = np.split(xdata.reshape( batch_size, -1),\n",
    "                          num_batches, 1)\n",
    "y_batches = np.split(ydata.reshape(batch_size, -1),\n",
    "                          num_batches, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建一个生成器,生成批次数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch():\n",
    "    x, y = x_batches[pointer], y_batches[pointer]\n",
    "    return x, y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib import legacy_seq2seq\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建LSTM的cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = []\n",
    "for _ in range(num_layers):\n",
    "    cell = rnn.LSTMCell(rnn_size)\n",
    "    if output_keep_prob < 1.0 or input_keep_prob < 1.0:\n",
    "        cell = rnn.DropoutWrapper(cell,\n",
    "                                  input_keep_prob=input_keep_prob,\n",
    "                                  output_keep_prob=output_keep_prob)\n",
    "    cells.append(cell)\n",
    "\n",
    "cell = rnn.MultiRNNCell(cells, state_is_tuple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化占位符,随机化参数矩阵，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "targets = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "with tf.variable_scope('rnnlm'):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\",[rnn_size, vocab_size])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将input转化为词嵌入向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])\n",
    "inputs = tf.nn.embedding_lookup(embedding, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拆散input_data放入rnn模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.split(inputs, seq_length, 1)\n",
    "inputs = [tf.squeeze(input_, [1]) for input_ in inputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decoder的输出和最终状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, last_state = legacy_seq2seq.rnn_decoder(inputs, initial_state, cell, scope='rnnlm')\n",
    "output = tf.reshape(tf.concat(outputs, 1), [-1, rnn_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对输出层做softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "probs = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = legacy_seq2seq.sequence_loss_by_example(\n",
    "        [logits],\n",
    "        [tf.reshape(targets, [-1])],\n",
    "        [tf.ones([batch_size * seq_length])])\n",
    "with tf.name_scope('cost'):\n",
    "    cost = tf.reduce_sum(loss) / batch_size / seq_length\n",
    "final_state = last_state\n",
    "lr = tf.Variable(0.0, trainable=False)\n",
    "tvars = tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),grad_clip)\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_result = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    # restore model\n",
    "    if init_from is not None:\n",
    "        saver.restore(sess, ckpt)\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        sess.run(tf.assign(lr,learning_rate * (decay_rate ** i)))\n",
    "        state = sess.run(initial_state)\n",
    "        pointer = 0\n",
    "        for j in range(num_batches):\n",
    "            start = time.time()\n",
    "            x, y = next_batch()\n",
    "            pointer +=1\n",
    "            feed = {input_data: x, targets: y}\n",
    "            \n",
    "            for a, (c, h) in enumerate(initial_state):\n",
    "                feed[c] = state[a].c\n",
    "                feed[h] = state[a].h\n",
    "\n",
    "      \n",
    "            train_loss, state, _ = sess.run([ cost, final_state,train_op], feed)\n",
    "            train_loss_result.append(train_loss)\n",
    "\n",
    "            end = time.time()\n",
    "            print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\"\n",
    "                  .format(i * num_batches + j,\n",
    "                          num_epochs * num_batches,\n",
    "                          i, train_loss, end - start))\n",
    "            if (i * num_batches + j) % save_every == 0\\\n",
    "                    or (i == num_epochs-1 and\n",
    "                        j == num_batches-1):\n",
    "                # save for the last result\n",
    "                checkpoint_path = os.path.join(save_dir, 'model.ckpt')\n",
    "                saver.save(sess, checkpoint_path,\n",
    "                           global_step=i * num_batches + j)\n",
    "                print(\"model saved to {}\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "_x = [i for i in range(1,len(train_loss_result)+1)]\n",
    "plt.plot(_x, train_loss_result, 'k-', label='Train Loss')\n",
    "plt.title('Cross Entropy Loss per Generation')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用语言模型做测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import cPickle\n",
    "with open(vocab_file, 'rb') as f:\n",
    "    chars, vocab = cPickle.load(f)\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib import legacy_seq2seq\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "batch_size = 1\n",
    "seq_length = 1\n",
    "\n",
    "cell = rnn.MultiRNNCell([ rnn.LSTMCell(rnn_size) for _ in range(num_layers)], state_is_tuple=True)\n",
    "\n",
    "input_data = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "targets = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "with tf.variable_scope('rnnlm'):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\",[rnn_size, vocab_size])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "    \n",
    "embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])\n",
    "inputs = tf.nn.embedding_lookup(embedding, input_data)\n",
    "\n",
    "inputs = tf.split(inputs, seq_length, 1)\n",
    "inputs = [tf.squeeze(input_, [1]) for input_ in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(prev, _):\n",
    "    prev = tf.matmul(prev, softmax_w) + softmax_b\n",
    "    prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "    return tf.nn.embedding_lookup(embedding, prev_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, last_state = legacy_seq2seq.rnn_decoder(inputs, initial_state, cell, loop_function=loop , scope='rnnlm')\n",
    "output = tf.reshape(tf.concat(outputs, 1), [-1, rnn_size])\n",
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "probs = tf.nn.softmax(logits)\n",
    "loss = legacy_seq2seq.sequence_loss_by_example(\n",
    "        [logits],\n",
    "        [tf.reshape(targets, [-1])],\n",
    "        [tf.ones([batch_size * seq_length])])\n",
    "with tf.name_scope('cost'):\n",
    "    cost = tf.reduce_sum(loss) / batch_size / seq_length\n",
    "final_state = last_state\n",
    "lr = tf.Variable(0.0, trainable=False)\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),grad_clip)\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample(sess, chars, vocab, num=200, prime='The ', sampling_type=1):\n",
    "    state = sess.run(cell.zero_state(1, tf.float32))\n",
    "    for char in prime[:-1]:\n",
    "        x = np.zeros((1, 1))\n",
    "        x[0, 0] = vocab[char]\n",
    "        feed = {input_data: x, initial_state: state}\n",
    "        [state] = sess.run([final_state], feed)\n",
    "\n",
    "    def weighted_pick(weights):\n",
    "        t = np.cumsum(weights)\n",
    "        s = np.sum(weights)\n",
    "        return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "    ret = prime\n",
    "    char = prime[-1]\n",
    "    for _ in range(num):\n",
    "        x = np.zeros((1, 1))\n",
    "        x[0, 0] = vocab[char]\n",
    "        feed = {input_data: x, initial_state: state}\n",
    "        [probs, state] = sess.run([probs, final_state], feed)\n",
    "        p = probs[0]\n",
    "\n",
    "        if sampling_type == 0:\n",
    "            sample = np.argmax(p)\n",
    "        elif sampling_type == 2:\n",
    "            if char == ' ':\n",
    "                sample = weighted_pick(p)\n",
    "            else:\n",
    "                sample = np.argmax(p)\n",
    "        else:  # sampling_type == 1 default:\n",
    "            sample = weighted_pick(p)\n",
    "\n",
    "        pred = chars[sample]\n",
    "        ret += pred\n",
    "        char = pred\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prime = '悲伤逆流成河'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(sample(sess, chars, vocab, 500, prime,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
