{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuxinxin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-41f12836106f>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/liuxinxin/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/liuxinxin/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /Users/liuxinxin/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /Users/liuxinxin/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/liuxinxin/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\n",
    "    \"/tmp/tensorflow/mnist/input_data\", \n",
    "    reshape=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "filename = \"mnist\"\n",
    "image = mnist.train.images[0]\n",
    "image_label = mnist.train.labels[0]\n",
    "_, rows, cols, depth = mnist.train.images.shape\n",
    "\n",
    "with tf.python_io.TFRecordWriter(filename) as writer:\n",
    "    image_raw = image.tostring()\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'height': _int64_feature(rows),\n",
    "        'width': _int64_feature(cols),\n",
    "        'depth': _int64_feature(depth),\n",
    "        'label': _int64_feature(int(image_label)),\n",
    "        'image_raw': _bytes_feature(image_raw)\n",
    "    }))\n",
    "    writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _data_path(data_directory:str, name:str) -> str:\n",
    "    \"\"\"Construct a full path to a TFRecord file to be stored in the \n",
    "    data_directory. Will also ensure the data directory exists\n",
    "    \n",
    "    Args:\n",
    "        data_directory: The directory where the records will be stored\n",
    "        name:           The name of the TFRecord\n",
    "    \n",
    "    Returns:\n",
    "        The full path to the TFRecord file\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    return os.path.join(data_directory, f'{name}.tfrecords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing mnist/validation.tfrecords data\n",
      "Processing sample 5000 of 5000\n",
      "Processing mnist/train-1.tfrecords data\n",
      "Processing sample 5500 of 5500\n",
      "Processing mnist/train-2.tfrecords data\n",
      "Processing sample 5500 of 5500\n",
      "Processing mnist/train-3.tfrecords data\n",
      "Processing sample 5500 of 5500\n",
      "Processing mnist/train-4.tfrecords data\n",
      "Processing sample 5500 of 5500\n",
      "Processing mnist/train-5.tfrecords data\n",
      "Processing sample 5500 of 5500\n",
      "Processing mnist/train-6.tfrecords data\n",
      "Processing sample 5500 of 5500\n",
      "Processing mnist/train-8.tfrecords data\n",
      "Processing sample 5500 of 5500\n",
      "Processing mnist/train-9.tfrecords data\n",
      "Processing sample 5500 of 5500\n",
      "Processing mnist/train-10.tfrecords data\n",
      "Processing sample 10000 of 10000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "def convert_to(data_set, name:str, data_directory:str, num_shards:int=1):\n",
    "    \"\"\"Convert the dataset into TFRecords on disk\n",
    "    \n",
    "    Args:\n",
    "        data_set:       The MNIST data set to convert\n",
    "        name:           The name of the data set\n",
    "        data_directory: The directory where records will be stored\n",
    "        num_shards:     The number of files on disk to separate records into\n",
    "    \"\"\"\n",
    "\n",
    "    num_examples, rows, cols, depth = data_set.images.shape\n",
    "\n",
    "    data_set = list(zip(data_set.images, data_set.labels))\n",
    "\n",
    "    def _process_examples(example_dataset, filename:str):\n",
    "        print(f'Processing {filename} data')\n",
    "        dataset_length = len(example_dataset)\n",
    "        with tf.python_io.TFRecordWriter(filename) as writer:\n",
    "            for index, (image, label) in enumerate(example_dataset):\n",
    "                sys.stdout.write(f\"\\rProcessing sample {index+1} of {dataset_length}\")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                image_raw = image.tostring()\n",
    "                example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                    'height': _int64_feature(rows),\n",
    "                    'width': _int64_feature(cols),\n",
    "                    'depth': _int64_feature(depth),\n",
    "                    'label': _int64_feature(int(label)),\n",
    "                    'image_raw': _bytes_feature(image_raw)\n",
    "                }))\n",
    "                writer.write(example.SerializeToString())\n",
    "            print()\n",
    "    \n",
    "    if num_shards == 1:\n",
    "        _process_examples(data_set, _data_path(data_directory, name))\n",
    "    else:\n",
    "        sharded_dataset = np.array_split(data_set, num_shards)\n",
    "        for shard, dataset in enumerate(sharded_dataset):\n",
    "            _process_examples(dataset, _data_path(data_directory, f'{name}-{shard+1}'))\n",
    "\n",
    "data_directory = \"mnist\"\n",
    "convert_to(mnist.validation, 'validation', data_directory)\n",
    "convert_to(mnist.train, 'train', data_directory, num_shards=10)\n",
    "convert_to(mnist.test, 'test', data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_input_fn(filenames, batch_size=1000, shuffle=False):\n",
    "    \n",
    "    def _parse(record):\n",
    "        features={\n",
    "            'label': tf.FixedLenFeature([], tf.int64),\n",
    "            'image_raw': tf.FixedLenFeature([], tf.string)\n",
    "        }\n",
    "        parsed_record = tf.parse_single_example(record, features)\n",
    "        image = tf.decode_raw(parsed_record['image_raw'], tf.float32)\n",
    "\n",
    "        label = tf.cast(parsed_record['label'], tf.int32)\n",
    "\n",
    "        return image, label\n",
    "        \n",
    "    def _input_fn():\n",
    "        dataset = (tf.contrib.data.TFRecordDataset(filenames)\n",
    "            .map(_parser))\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=10_000)\n",
    "\n",
    "        dataset = dataset.repeat(None) # Infinite iterations: let experiment determine num_epochs\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        \n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        features, labels = iterator.get_next()\n",
    "        \n",
    "        return features, labels\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = data_input_fn(glob.glob('/path/to/data/train-*.tfrecords'), shuffle=True)\n",
    "validation_input_fn = data_input_fn('/path/to/data/validation.tfrecords')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
